# llm_pos/wilcox/run_wilcox_aggregation.py
import pandas as pd
import os
import math
from transformers import AutoTokenizer

# --- Configuration ---
# Assume this script is in llm_pos/wilcox/
CURRENT_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Define base paths relative to this script's location
DATA_INPUT_DIR = os.path.join(CURRENT_SCRIPT_DIR, "tims_data") # For word_region_mapping & sentence_components
BPE_RESULTS_INPUT_DIR = os.path.join(CURRENT_SCRIPT_DIR, "tims_results") # BPE surprisals from your other script
AGGREGATED_RESULTS_OUTPUT_DIR = os.path.join(CURRENT_SCRIPT_DIR, "tims_results") # Where this script saves output

MODEL_NAME_FOR_FILES = "gpt2"  # Matches your tim_extract_surprisals.py
HF_MODEL_NAME_TOKENIZER = "gpt2" # Tokenizer should match the model used for BPEs

# INPUT FILES (ensure these exist in DATA_INPUT_DIR or BPE_RESULTS_INPUT_DIR)
# This is the CSV you use as input for tim_extract_surprisals.py;
# It MUST contain 'item', 'condition', and also 'prep', 'np2' columns if critical region logic is to work.
SENTENCE_COMPONENTS_INPUT_CSV_BASENAME = "test_set.csv" # e.g., your test_set.csv or my_stimuli.csv
SENTENCE_COMPONENTS_INPUT_CSV = os.path.join(DATA_INPUT_DIR, SENTENCE_COMPONENTS_INPUT_CSV_BASENAME)

# This is the BPE-level surprisal CSV generated by tim_extract_surprisals.py
BPE_SURPRISALS_CSV_BASENAME = f"{SENTENCE_COMPONENTS_INPUT_CSV_BASENAME.split('.')[0]}_surprisals_{MODEL_NAME_FOR_FILES}.csv"
BPE_SURPRISALS_CSV = os.path.join(BPE_RESULTS_INPUT_DIR, BPE_SURPRISALS_CSV_BASENAME)

WORD_REGION_MAPPING_CSV_BASENAME = "word_region_mapping.csv" # You'll need this file
WORD_REGION_MAPPING_CSV = os.path.join(DATA_INPUT_DIR, WORD_REGION_MAPPING_CSV_BASENAME)

# OUTPUT FILE for this script
AGGREGATED_OUTPUT_CSV = os.path.join(AGGREGATED_RESULTS_OUTPUT_DIR, f"{MODEL_NAME_FOR_FILES}_critical_region_surprisals_aggregated.csv")

# --- Initialize Tokenizer ---
try:
    print(f"Loading tokenizer: {HF_MODEL_NAME_TOKENIZER}")
    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME_TOKENIZER)
    # GPT-2 tokenizer usually has eos_token as pad_token by default if pad_token is not set.
    if tokenizer.pad_token is None:
        if tokenizer.eos_token is not None:
            print(f"Tokenizer: Setting pad_token to eos_token ('{tokenizer.eos_token}')")
            tokenizer.pad_token = tokenizer.eos_token
        else:
            print("Tokenizer: Adding new pad_token '<|pad|>'")
            tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
except Exception as e:
    print(f"Critical Error: Could not load tokenizer '{HF_MODEL_NAME_TOKENIZER}'. Exception: {e}")
    exit()

def main():
    # Create output directory if it doesn't exist
    if not os.path.exists(AGGREGATED_RESULTS_OUTPUT_DIR):
        os.makedirs(AGGREGATED_RESULTS_OUTPUT_DIR)
        print(f"Created output directory: {AGGREGATED_RESULTS_OUTPUT_DIR}")

    input_files_to_check = {
        "Sentence Components CSV": SENTENCE_COMPONENTS_INPUT_CSV,
        "BPE Surprisals CSV": BPE_SURPRISALS_CSV,
        "Word Region Mapping CSV": WORD_REGION_MAPPING_CSV
    }
    missing_files = False
    for name, fpath in input_files_to_check.items():
        if not os.path.exists(fpath):
            print(f"Critical Error: Input file '{name}' not found at '{fpath}'")
            missing_files = True
    if missing_files:
        print("Please ensure all required input files are present and paths are correct in the script.")
        return

    print("Loading data...")
    bpe_df = pd.read_csv(BPE_SURPRISALS_CSV)
    word_map_df = pd.read_csv(WORD_REGION_MAPPING_CSV)
    components_df = pd.read_csv(SENTENCE_COMPONENTS_INPUT_CSV)
    print("Data loaded.")
    
    print(f"Columns in BPE Surprisals file ('{os.path.basename(BPE_SURPRISALS_CSV)}'): {bpe_df.columns.tolist()}")
    print(f"Columns in Word Region Mapping file ('{os.path.basename(WORD_REGION_MAPPING_CSV)}'): {word_map_df.columns.tolist()}")
    print(f"Columns in Sentence Components file ('{os.path.basename(SENTENCE_COMPONENTS_INPUT_CSV)}'): {components_df.columns.tolist()}")


    # Standardize 'source_doc_name' in word_map_df if 'test' column exists
    if 'test' in word_map_df.columns and 'source_doc_name' not in word_map_df.columns:
        word_map_df.rename(columns={'test': 'source_doc_name'}, inplace=True)
        print(f"Renamed 'test' to 'source_doc_name' in {WORD_REGION_MAPPING_CSV_BASENAME}")
    elif 'test' not in word_map_df.columns and 'source_doc_name' not in word_map_df.columns:
        print(f"Critical Error: Missing 'source_doc_name' or 'test' column in {WORD_REGION_MAPPING_CSV_BASENAME}. Cannot proceed with merging.")
        return

    # Ensure join keys are strings
    for df in [bpe_df, word_map_df, components_df]:
        for col in ['item', 'condition']:
            if col in df.columns:
                df[col] = df[col].astype(str)
            else:
                print(f"Warning: Column '{col}' not found in one of the DataFrames. This might affect grouping/merging.")
    if 'source_doc_name' in bpe_df.columns: bpe_df['source_doc_name'] = bpe_df['source_doc_name'].astype(str)
    if 'source_doc_name' in word_map_df.columns: word_map_df['source_doc_name'] = word_map_df['source_doc_name'].astype(str)
    # components_df source_doc_name is used if present for merging; handled by groupby key


    print("\n--- Source Document Name Diagnostics ---")
    bpe_unique_sources = bpe_df['source_doc_name'].unique() if 'source_doc_name' in bpe_df.columns else ["Column Missing"]
    word_map_unique_sources = word_map_df['source_doc_name'].unique() if 'source_doc_name' in word_map_df.columns else ["Column Missing"]
    print(f"Unique source_doc_name in BPE surprisals file: {bpe_unique_sources}")
    print(f"Unique source_doc_name in word mapping file: {word_map_unique_sources}")
    print("-------------------------------------\n")

    aggregated_results = []
    # Determine the correct surprisal column name from BPE data
    surprisal_col_name = f"surprisal_bits_{MODEL_NAME_FOR_FILES}" # As output by your tim_extract_surprisals.py
    
    if surprisal_col_name not in bpe_df.columns:
        print(f"Critical Error: Surprisal column '{surprisal_col_name}' not found in {BPE_SURPRISALS_CSV}.")
        print(f"Available columns: {bpe_df.columns.tolist()}")
        return
    
    # Check for bpe_token_str, as bpe_token_id will be N/A
    if 'bpe_token_str' not in bpe_df.columns:
        print(f"Critical Error: Column 'bpe_token_str' not found in {BPE_SURPRISALS_CSV}. Needed for word reconstruction.")
        return

    grouped_bpe = bpe_df.groupby(['source_doc_name', 'item', 'condition'])
    grouped_word_map = word_map_df.groupby(['source_doc_name', 'item', 'condition'])

    print(f"Processing {len(grouped_bpe)} unique sentence groups from BPE file...")

    for name_bpe, bpe_group in grouped_bpe:
        source_doc_bpe, item_id, condition_val = name_bpe
        
        # Attempt to find matching group in word_map_df (handles if source_doc_name has .csv or not)
        word_map_group = None
        keys_to_try = [name_bpe]
        if isinstance(source_doc_bpe, str) and source_doc_bpe.endswith('.csv'):
            keys_to_try.append((source_doc_bpe[:-4], item_id, condition_val))
        
        for key_try in keys_to_try:
            try:
                word_map_group = grouped_word_map.get_group(key_try)
                break 
            except KeyError:
                continue
            
        if word_map_group is None:
            print(f"  Warning: No word mapping found for BPE group {name_bpe} (tried keys: {keys_to_try}). Skipping.")
            continue

        current_bpes = bpe_group.sort_values(by="bpe_token_index").to_dict('records')
        target_words_info = word_map_group.to_dict('records')

        sentence_word_surprisals = []
        bpe_cursor = 0
        
        for word_info in target_words_info:
            target_word_text = str(word_info['token']).strip()
            current_word_bpe_strings = [] # MODIFIED: Store BPE strings
            current_word_total_surprisal = 0
            reconstructed_word = ""
            
            start_bpe_cursor = bpe_cursor # For backtracking if needed

            while bpe_cursor < len(current_bpes):
                bpe_data = current_bpes[bpe_cursor]
                current_word_bpe_strings.append(bpe_data['bpe_token_str'])
                current_word_total_surprisal += bpe_data[surprisal_col_name]
                
                # MODIFIED: Reconstruct word from BPE strings
                try:
                    reconstructed_word = tokenizer.convert_tokens_to_string(current_word_bpe_strings).strip()
                except Exception as e:
                    print(f"    Error during tokenizer.convert_tokens_to_string for BPEs {current_word_bpe_strings}: {e}")
                    reconstructed_word = "TOKENIZER_ERROR" # Avoid crash, mark as error
                
                bpe_cursor += 1
                if reconstructed_word == target_word_text:
                    break
                # Heuristic to prevent over-consuming BPEs if a match is difficult
                if len(reconstructed_word) > len(target_word_text) + 5 and not target_word_text.startswith(reconstructed_word) and not reconstructed_word.startswith(target_word_text):
                    # Backtrack one BPE token
                    bpe_cursor -= 1
                    current_word_bpe_strings.pop()
                    current_word_total_surprisal -= bpe_data[surprisal_col_name]
                    reconstructed_word = tokenizer.convert_tokens_to_string(current_word_bpe_strings).strip()
                    break
            
            if reconstructed_word == target_word_text:
                sentence_word_surprisals.append({
                    'word': target_word_text,
                    'region_name': word_info['region'],
                    'surprisal': current_word_total_surprisal
                })
            else:
                print(f"  Warning: Word alignment failed for target '{target_word_text}' (reconstructed: '{reconstructed_word}') in group {name_bpe}. BPEs tried: {current_word_bpe_strings}. Surprisal set to NaN.")
                # Reset cursor for next target word to retry from where this word started
                bpe_cursor = start_bpe_cursor 
                sentence_word_surprisals.append({
                    'word': target_word_text,
                    'region_name': word_info['region'],
                    'surprisal': math.nan 
                })
        
        # --- Component Lookup (ensure 'item' and 'condition' match types) ---
        component_row_series = components_df[
             (components_df['item'].astype(str) == str(item_id)) & 
             (components_df['condition'].astype(str) == str(condition_val))
        ]
        
        if component_row_series.empty:
             print(f"  Warning: No component data found for item '{item_id}', condition '{condition_val}' in {os.path.basename(SENTENCE_COMPONENTS_INPUT_CSV)}. Skipping critical region aggregation for this group.")
             aggregated_critical_surprisal = math.nan # Mark as NaN if no component data
             final_critical_region_text = "N/A (no component data)"
        else:
            component_row = component_row_series.iloc[0]
            critical_region_text_parts = []
            target_region_names = []
            
            # Determine critical region based on condition (as per original script)
            # Ensure 'prep' and 'np2' columns exist in your SENTENCE_COMPONENTS_INPUT_CSV
            if condition_val.endswith("_gap"): # E.g., "what_gap", "that_gap"
                target_region_names = ["prep"] 
                prep_text = str(component_row.get('prep', "") or "").strip() # Handle missing or None
                if prep_text: critical_region_text_parts.append(prep_text)
            elif condition_val.endswith("_nogap"): # E.g., "what_nogap", "that_nogap"
                target_region_names = ["np2"] 
                np2_text = str(component_row.get('np2', "") or "").strip() # Handle missing or None
                if np2_text: critical_region_text_parts.append(np2_text)
            else:
                print(f"  Warning: Unknown condition format '{condition_val}' for {name_bpe}. Cannot determine critical region.")
                target_region_names = [] # No specific region
            
            final_critical_region_text = " ".join(critical_region_text_parts).strip()
            if not final_critical_region_text and target_region_names: # If regions defined but text is empty
                 final_critical_region_text = "N/A (empty component text)"
            elif not target_region_names:
                 final_critical_region_text = "N/A (no target region)"


            aggregated_critical_surprisal = 0.0 
            found_critical_words_count = 0
            expected_critical_words_count = sum(1 for w_info in target_words_info if w_info['region'] in target_region_names)

            if not target_region_names: # If no specific critical region defined by condition
                aggregated_critical_surprisal = math.nan # Or 0.0 if that's preferred for "no region"
            else:
                for word_sur_info in sentence_word_surprisals:
                    if word_sur_info['region_name'] in target_region_names:
                        if not math.isnan(word_sur_info['surprisal']):
                            aggregated_critical_surprisal += word_sur_info['surprisal']
                            found_critical_words_count += 1
                        else: # If any word in CR has NaN surprisal, the whole CR is NaN
                            aggregated_critical_surprisal = math.nan 
                            break 
                
                if not math.isnan(aggregated_critical_surprisal) and found_critical_words_count == 0 and expected_critical_words_count > 0 :
                    print(f"  Warning: Critical region '{final_critical_region_text}' (regions: {target_region_names}) for {name_bpe} had expected words but none were found/aggregated. Setting surprisal to NaN.")
                    aggregated_critical_surprisal = math.nan
                elif found_critical_words_count < expected_critical_words_count and expected_critical_words_count > 0:
                    print(f"  Warning: Not all expected words found for critical region '{final_critical_region_text}' (regions: {target_region_names}) for {name_bpe}. Found {found_critical_words_count}/{expected_critical_words_count}. Aggregated from found words.")


        aggregated_results.append({
            "source_doc_name": source_doc_bpe, # Use the one from BPE file, after potential reconciliation
            "item": item_id,
            "condition": condition_val,
            "critical_region_text": final_critical_region_text,
            "aggregated_surprisal_bits": aggregated_critical_surprisal
        })

    if aggregated_results:
        output_df = pd.DataFrame(aggregated_results)
        output_df.to_csv(AGGREGATED_OUTPUT_CSV, index=False, float_format='%.8f')
        print(f"\nSuccessfully aggregated surprisals to {AGGREGATED_OUTPUT_CSV}")
    else:
        print("\nNo results were successfully aggregated.")

if __name__ == "__main__":
    main()